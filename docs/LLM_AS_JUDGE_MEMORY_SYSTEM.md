# LLM-as-a-Judge Memory System

## Overview

The creativity agent now uses an **LLM-as-a-Judge** approach where the refinement agent explicitly evaluates, scores, and structures ideas for memory insertion. This replaces the previous heuristic-based scoring system with a more reliable, intelligent evaluation process.

## Key Improvements

### âœ… **Before (Heuristic-Based)**
- Memory manager used keyword matching and text patterns
- Scored based on word counts, technical markers, innovation buzzwords
- Prone to false positives (scoring headers as "ideas")
- No rejection of low-quality concepts
- No key points extraction
- Unreliable quality assessment

### âœ… **After (LLM-as-Judge)**
- **Refinement agent** evaluates and scores every idea
- Uses 4 evaluation criteria: Originality, Technical Feasibility, Impact Potential, Substance
- Explicitly rejects ideas scoring < 5.0 with reasoning
- Extracts detailed key points for each accepted idea
- Research-validated through mandatory web searches
- Structured output format for reliable parsing

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DIVERGENT AGENT (High Temp)                                â”‚
â”‚  - Generates creative ideas                                 â”‚
â”‚  - No evaluation, just exploration                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Raw creative output
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REFINEMENT AGENT (Low Temp) - LLM AS JUDGE                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ FOR EACH IDEA:                                      â”‚   â”‚
â”‚  â”‚ 1. Evaluate: Originality (0-10)                     â”‚   â”‚
â”‚  â”‚ 2. Evaluate: Technical Feasibility (0-10)           â”‚   â”‚
â”‚  â”‚ 3. Evaluate: Impact Potential (0-10)                â”‚   â”‚
â”‚  â”‚ 4. Evaluate: Substance (0-10)                       â”‚   â”‚
â”‚  â”‚ 5. Calculate: Overall Score (average)               â”‚   â”‚
â”‚  â”‚ 6. Decision: Accept (â‰¥5.0) or Reject (<5.0)         â”‚   â”‚
â”‚  â”‚ 7. Extract: 3-5 key points for accepted ideas       â”‚   â”‚
â”‚  â”‚ 8. Validate: Use search_web for research backing    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  OUTPUT FORMAT:                                             â”‚
â”‚  ### ACCEPTED IDEAS                                         â”‚
â”‚  ### IDEA: [name]                                           â”‚
â”‚  **Quality Score**: X.X/10                                  â”‚
â”‚  **Key Points**: - [point 1] - [point 2] ...               â”‚
â”‚                                                             â”‚
â”‚  ### REJECTED IDEAS                                         â”‚
â”‚  ### REJECTED: [name]                                       â”‚
â”‚  **Rejection Reasons**: - [reason 1] - [reason 2]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Structured, evaluated output
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEMORY MANAGER                                             â”‚
â”‚  - Parses structured LLM judge output                       â”‚
â”‚  - Extracts accepted ideas with scores & key points         â”‚
â”‚  - Extracts rejected ideas with rejection reasons           â”‚
â”‚  - No heuristics, just parsing                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IDEA MEMORY (Per-Run Persistence)                          â”‚
â”‚  â”œâ”€â”€ explored_ideas[]                                       â”‚
â”‚  â”‚   â”œâ”€â”€ concept: "Distributed Swarm Intelligence..."      â”‚
â”‚  â”‚   â”œâ”€â”€ quality_score: 8.5                                â”‚
â”‚  â”‚   â”œâ”€â”€ key_points: ["Cognitive ant colonies", ...]       â”‚
â”‚  â”‚   â””â”€â”€ iteration: 0                                      â”‚
â”‚  â””â”€â”€ rejected_ideas[]                                       â”‚
â”‚      â”œâ”€â”€ concept: "Revolutionary LLM Enhancement Ideas"     â”‚
â”‚      â”œâ”€â”€ reason: "Appears to be header; Too vague"         â”‚
â”‚      â””â”€â”€ iteration: 0                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Refinement Agent Prompt Structure

The refinement agent now follows a **mandatory structured format**:

### 1. Evaluation Criteria (Score 0-10 each)

```
Originality: How novel and unique?
  9-10: Groundbreaking, no precedent
  7-8:  Highly innovative
  5-6:  Moderate novelty
  3-4:  Mostly derivative
  0-2:  Common/trivial

Technical Feasibility: Can it be implemented?
  9-10: Proven technology, clear path
  7-8:  Feasible with current tech
  5-6:  Requires breakthroughs but plausible
  3-4:  Major hurdles
  0-2:  Technically implausible

Impact Potential: How significant are benefits?
  9-10: Transformative, paradigm-shifting
  7-8:  Major improvements
  5-6:  Moderate improvements
  3-4:  Minor gains
  0-2:  Negligible impact

Substance: How well-developed?
  9-10: Comprehensive with implementation details
  7-8:  Well-developed with key components
  5-6:  Decent outline
  3-4:  Vague with limited specifics
  0-2:  Too abstract
```

### 2. Structured Output Format

**For Accepted Ideas (score â‰¥ 5.0):**
```markdown
### IDEA: [Concise, specific name]

**Quality Score**: 8.5/10

**Evaluation Breakdown**:
- Originality: 9/10 - Unique combination of swarm intelligence and LLMs
- Technical Feasibility: 8/10 - Proven ant colony optimization algorithms exist
- Impact Potential: 9/10 - Could revolutionize distributed AI processing
- Substance: 8/10 - Well-defined architecture with clear components

**Description**:
[2-4 sentence comprehensive description]

**Key Points**:
- Cognitive ant colonies with specialized neural agents
- Dynamic load balancing through digital pheromone trails
- Fault-tolerant communication mechanisms
- Emergent intelligence from decentralized decision-making
- 40% reduction in inference latency (validated)

**Research Validation**:
[Results from search_web tool showing real-world evidence]
```

**For Rejected Ideas (score < 5.0):**
```markdown
### REJECTED: Revolutionary LLM Enhancement Ideas for the Future

**Quality Score**: 2.0/10 (REJECTED)

**Rejection Reasons**:
- Appears to be a header, not an actual idea
- Too vague and generic with no specific technical content
- Lacks substance and implementation details
- No originality (just a title)
```

### 3. Mandatory Web Research

The refinement agent **MUST** use the `search_web` tool to:
- Validate technical feasibility
- Find precedents and case studies
- Research market validation
- Identify obstacles and challenges
- Gather supporting evidence

Example searches:
```python
search_web("swarm intelligence AI systems real-world applications 2024", max_results=5)
search_web("quantum-classical hybrid architectures feasibility", max_results=5)
search_web("mixture of experts LLM challenges limitations", max_results=5)
```

## Memory Manager Implementation

### Divergent Output (High-Temp)
- Uses simple extraction (no evaluation)
- Just captures main idea names
- Default score: 6.0 (moderate)
- No key points extraction
- Minimal processing

### Refined Output (Low-Temp) 
- Parses structured LLM judge format
- Extracts accepted ideas with:
  - Quality scores from refinement agent
  - Key points list (up to 5)
  - Concept name
- Extracts rejected ideas with:
  - Rejection reasons
  - Failed evaluation criteria

### Parsing Logic

```python
def _parse_accepted_ideas(text):
    # 1. Find "### ACCEPTED IDEAS" section
    # 2. Split by "### IDEA:" markers
    # 3. Extract quality score from "**Quality Score**: X.X/10"
    # 4. Extract key points from "**Key Points**:" bullet list
    # 5. Return structured list of ideas
    
def _parse_rejected_ideas(text):
    # 1. Find "### REJECTED IDEAS" section
    # 2. Split by "### REJECTED:" markers
    # 3. Extract rejection reasons from "**Rejection Reasons**:" list
    # 4. Return structured list of rejections
```

## Benefits

### ðŸŽ¯ **Accuracy**
- LLM evaluation is far more nuanced than keyword matching
- Understands context, novelty, and technical depth
- Reduces false positives (headers marked as ideas)

### ðŸŽ¯ **Transparency**
- Explicit scoring criteria visible to users
- Clear rejection reasons
- Traceable evaluation process

### ðŸŽ¯ **Validation**
- Mandatory web research backing
- Real-world feasibility checks
- Evidence-based scoring

### ðŸŽ¯ **Quality Control**
- Automatic rejection of low-quality ideas
- Key points extraction ensures substance
- Prevents meta-content from polluting memory

### ðŸŽ¯ **Simplicity**
- Memory manager just parses, doesn't evaluate
- Single source of truth (refinement agent)
- Less code, more reliable

## Example Memory Output

**Before (Heuristic System):**
```json
{
  "explored_ideas": [
    {
      "concept": "## Revolutionary LLM Enhancement Ideas for the Future",
      "key_points": [],
      "quality_score": 6.0  // WRONG - this is a header!
    },
    {
      "concept": "Executive Summary",
      "key_points": [],
      "quality_score": 7.5  // WRONG - meta-content!
    }
  ],
  "rejected_ideas": []  // No rejections ever!
}
```

**After (LLM-as-Judge):**
```json
{
  "explored_ideas": [
    {
      "concept": "Distributed Swarm Intelligence Architecture",
      "key_points": [
        "Cognitive ant colonies with specialized neural agents",
        "Dynamic load balancing through pheromone trails",
        "40% reduction in inference latency",
        "Fault-tolerant decentralized communication",
        "Emergent intelligence from agent coordination"
      ],
      "quality_score": 8.5,
      "iteration": 0
    },
    {
      "concept": "Quantum-Classical Hybrid Reasoning Engine",
      "key_points": [
        "Quantum processors handle optimization problems",
        "Classical systems manage language understanding",
        "Proven feasibility in quantum-enhanced ML",
        "Hybrid architecture reduces quantum resource requirements"
      ],
      "quality_score": 7.2,
      "iteration": 0
    }
  ],
  "rejected_ideas": [
    {
      "concept": "Revolutionary LLM Enhancement Ideas for the Future",
      "reason": "Appears to be a header; Too vague; No technical substance",
      "iteration": 0
    },
    {
      "concept": "Executive Summary",
      "reason": "Meta-content, not an actual idea; Insufficient originality",
      "iteration": 0
    }
  ]
}
```

## Configuration

### Rejection Threshold
```python
# In memory_manager.py
REJECTION_THRESHOLD = 5.0  # Ideas scoring below this are rejected
```

### Max Concepts per Iteration
```python
# In agent_flow.py - extract_concepts_from_text call
max_concepts=20  # Can be increased if needed
```

### Key Points Limit
```python
# Automatically limited to 5 most important points
key_points=concept_data['key_points'][:5]
```

## Future Enhancements

1. **Meta-Analysis Agent**: After all iterations, create comprehensive research report on top ideas
2. **Comparative Scoring**: Compare ideas across iterations for evolution tracking
3. **Confidence Intervals**: Add confidence levels to quality scores
4. **Domain-Specific Rubrics**: Custom evaluation criteria per domain
5. **Multi-Judge Consensus**: Use multiple agents for scoring consistency

## Migration Notes

### Breaking Changes
- Old heuristic-based memory files won't have key points or rejection tracking
- Quality scores may differ from previous heuristic calculations
- Refinement agent output format has changed

### Backward Compatibility
- Divergent (high-temp) output still uses simple extraction for compatibility
- Existing memory files can be loaded but won't have new structure
- Recommend starting fresh run for new memory format

## Testing

Run a test to verify the LLM-as-Judge system:

```bash
cd creativity_agent
uv run python main.py --prompt "Future LLM enhancements" --iterations 2
```

Check the memory file:
```bash
cat outputs/run_*/memory/idea_memory.json
```

Verify:
- âœ… No headers or meta-content in explored_ideas
- âœ… All accepted ideas have quality_score â‰¥ 5.0
- âœ… All accepted ideas have 3-5 key_points
- âœ… Rejected ideas exist with clear rejection reasons
- âœ… Quality scores reflect actual substance and originality

## Summary

The LLM-as-a-Judge approach transforms the memory system from **brittle pattern matching** to **intelligent evaluation**. The refinement agent becomes both a synthesizer AND a quality judge, ensuring only substantive, well-developed ideas make it into memory while transparently rejecting low-quality concepts with clear reasoning.
